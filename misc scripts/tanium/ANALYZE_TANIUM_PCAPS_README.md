# Tanium Download Performance Analysis - Complete Metrics Guide

## Overview

This comprehensive guide explains how to use **`analyze_tanium_pcaps.py`** to diagnose Tanium CDN vs Legacy download performance issues.

### What This Guide Covers

- **Tool**: `analyze_tanium_pcaps.py` - The packet capture analysis script
- **Input**: PCAPs generated by `tanium_download_perf_test.py`
- **Purpose**: Compare CDN vs Legacy download performance and identify throttling

### Expected PCAP Structure

The analysis script expects PCAPs from the performance test tool:

```bash
# Generate PCAPs using the performance test
python3 tanium_download_perf_test.py

# This creates a directory with PCAPs:
tanium_pcaps_YYYYMMDD_HHMMSS/
├── legacy_iteration_1.pcap
├── legacy_iteration_2.pcap
├── legacy_iteration_3.pcap
├── legacy_iteration_4.pcap
├── legacy_iteration_5.pcap
├── cdn_iteration_1.pcap
├── cdn_iteration_2.pcap
├── cdn_iteration_3.pcap
├── cdn_iteration_4.pcap
└── cdn_iteration_5.pcap

# Analyze the PCAPs
python3 analyze_tanium_pcaps.py --pcap-dir tanium_pcaps_YYYYMMDD_HHMMSS/
```

**File naming requirements:**
- Files containing `legacy` are categorized as Legacy server downloads (port 17472)
- Files containing `cdn` are categorized as CDN downloads (port 443)
- Multiple iterations (typically 5 each) for statistical significance

This guide explains TCP fundamentals, every metric measured, the mathematics behind calculations, and how to interpret results.

---

## Table of Contents

1. [TCP Fundamentals](#tcp-fundamentals)
2. [Download Scenarios Explained](#download-scenarios-explained)
3. [Metric Categories](#metric-categories)
4. [Basic Statistics](#basic-statistics)
5. [Connection Establishment](#connection-establishment)
6. [Round-Trip Time (RTT)](#round-trip-time-rtt)
7. [Packet Loss & Retransmissions](#packet-loss--retransmissions)
8. [TCP Flow Control & Issues](#tcp-flow-control--issues)
9. [Throughput Analysis](#throughput-analysis)
10. [Root Cause Metrics](#root-cause-metrics)
    - [Server Response Latency](#server-response-latency)
    - [Inter-Packet Delays](#inter-packet-delays)
    - [Burst Pattern Analysis](#burst-pattern-analysis)
    - [Pacing Precision Analysis](#pacing-precision-analysis)
11. [Statistical Methods](#statistical-methods)
12. [Diagnosis Logic](#diagnosis-logic)
13. [Rate Limiting Algorithms](#rate-limiting-algorithms)
14. [CDN vs Legacy Comparison](#cdn-vs-legacy-comparison)
15. [Case Studies](#case-studies)
16. [Troubleshooting Guide](#troubleshooting-guide)
17. [Appendix: Technical Details](#appendix-technical-details)

---

## TCP Fundamentals

Before diving into metrics, let's establish TCP basics relevant to this analysis.

### TCP Connection Model

In a download scenario:
- **Client**: Initiates connection, receives data (Tanium client)
- **Server**: Accepts connection, sends data (Legacy server or CDN)
- **Download**: Server → Client data transfer (server srcport, client dstport)
- **ACKs**: Client → Server acknowledgments (client srcport, server dstport)

### TCP Window and Flow Control

**TCP Windows control data flow:**

1. **Receive Window** (advertised by receiver):
   - "I can receive X bytes before my buffer fills"
   - Receiver advertises this in every packet
   - In downloads: **Client advertises** its receive window

2. **Congestion Window** (sender's internal state):
   - "I will send X bytes before waiting for ACK"
   - Not visible in packets (internal TCP state)
   - Sender manages this based on network conditions

**Key Insight:** In a download, the **client's window** matters because it limits how much data the server can send. The **server's window** is largely irrelevant because the client only sends small ACKs.

### TCP Packet Structure

```
Ethernet | IP | TCP Header | TCP Data
                   |
                   +-- Source Port
                   +-- Dest Port
                   +-- Sequence Number
                   +-- ACK Number
                   +-- Flags (SYN, ACK, FIN, RST, PSH)
                   +-- Window Size
                   +-- Checksum
```

### What We Can Measure

From packet captures we can see:
- **Timing**: When packets are sent/received
- **Size**: How much data in each packet
- **Sequence numbers**: Order and retransmissions
- **Window sizes**: Flow control state
- **Flags**: Connection state and issues

What we **cannot** see directly:
- Server CPU/memory usage
- Application-level decisions
- Why the server chose specific timing

**But** we can **infer** server behavior from timing patterns.

---

## Download Scenarios Explained

### Scenario: Legacy Server Download

```
Client                           Legacy Server (port 17472)
  |                                     |
  |--- SYN ------------------------->  |
  |<-- SYN-ACK ---------------------   |
  |--- ACK ------------------------->  |
  |--- GET /file -------------------->  |
  |                                     |
  |<== DATA DATA DATA DATA DATA DATA == | (Continuous stream)
  |<== DATA DATA DATA DATA DATA DATA == |
  |<== DATA DATA DATA DATA DATA DATA == |
  |--- ACK ACK ACK ------------------>  |
  |<== DATA DATA DATA DATA DATA DATA == |
  |                                     |
```

**Characteristics:**
- Server sends data **continuously**
- Minimal gaps between packets (~0.1-1ms)
- Full-sized packets (1460 bytes TCP data)
- Smooth, consistent throughput

### Scenario: Throttled CDN Download

```
Client                           CDN Server (port 443)
  |                                     |
  |--- SYN ------------------------->  |
  |<-- SYN-ACK ---------------------   |
  |--- ACK ------------------------->  |
  |--- GET /file -------------------->  |
  |                                     |
  |<== DATA DATA DATA === [20ms gap]   | (Burst 1)
  |<== DATA DATA DATA === [20ms gap]   | (Burst 2)
  |<== DATA DATA DATA === [20ms gap]   | (Burst 3)
  |--- ACK ACK ACK ------------------>  |
  |<== DATA DATA DATA === [20ms gap]   | (Burst 4)
  |                                     |
```

**Characteristics:**
- Server sends in **bursts** with gaps
- Consistent gap timing (~20ms between bursts)
- Each burst has 3-5 packets
- Highly variable throughput

**This is the smoking gun for throttling.**

---

## Metric Categories

We organize metrics into three categories:

### 1. Outcome Metrics (What Happened)
- Basic statistics (throughput, duration, bytes)
- Throughput over time

**Purpose:** Quantify the performance problem

### 2. Network Metrics (Path Quality)
- RTT (network latency)
- Packet loss (retransmissions)
- TCP flags (connection issues)

**Purpose:** Rule out network as the bottleneck

### 3. Root Cause Metrics (Why It Happened)
- Inter-packet delays
- Burst patterns
- Pacing precision
- Server response latency

**Purpose:** Identify server-side throttling behavior

---

## Basic Statistics

### Metrics Collected

```python
{
  "total_packets": 8532,
  "duration_seconds": 45.23,
  "total_bytes": 12582912,
  "avg_throughput_mbps": 2.23
}
```

### Calculations

**Average Throughput:**
```
Throughput (Mbps) = (total_bytes × 8) / (duration_seconds × 1,000,000)
```

Example:
```
12,582,912 bytes × 8 bits/byte = 100,663,296 bits
100,663,296 / (45.23 × 1,000,000) = 2.23 Mbps
```

### Implementation Details

We use tshark to extract:
```bash
tshark -r file.pcap -T fields -Y tcp \
  -e frame.number \
  -e frame.time_relative \
  -e frame.len
```

This gives us:
- Frame number (packet count)
- Relative timestamp (duration = last timestamp)
- Frame length (sum = total bytes)

### What This Tells Us

**Throughput interpretation:**
- **>100 Mbps**: Good, server/network performing well
- **50-100 Mbps**: Moderate, some limitation
- **20-50 Mbps**: Poor, significant bottleneck
- **<20 Mbps**: Very poor, major issue

**But this is just the outcome.** We need other metrics to know WHY.

### CDN vs Legacy Expected Values

Typical observations:
- Legacy: 100-200 Mbps (wire speed, minimal delays)
- CDN (throttled): 20-60 Mbps (rate limited)

**Red flag:** If CDN throughput is <50% of Legacy, investigate root causes.

---

## Connection Establishment

### What We Measure

The **TCP three-way handshake** timing:

```
Client                Server
  |                     |
  |--- SYN -----------> | (t0)
  |<-- SYN-ACK -------- | (t1)
  |--- ACK -----------> | (t2)
```

**Handshake Time** = t1 - t0 (SYN to SYN-ACK)

### Metrics Collected

```python
{
  "handshake_time_ms": 12.34,
  "syn_time": 0.0,
  "synack_time": 0.01234
}
```

### Why This Matters

Handshake time tells us:
- **Network latency**: Minimum ~RTT/2 for the path
- **Server responsiveness**: How quickly server accepts connections
- **Connection overhead**: Fixed cost before any data transfer

### Interpretation

- **<10ms**: Excellent, low latency path
- **10-30ms**: Good, typical for regional connections
- **30-100ms**: Moderate, geographic distance or routing
- **>100ms**: High latency, international or routing issues

### Implementation

```bash
tshark -r file.pcap -T fields -Y "tcp.flags.syn == 1" \
  -e frame.time_relative \
  -e tcp.flags.syn \
  -e tcp.flags.ack
```

We look for:
1. First packet with SYN=1, ACK=0 (client SYN)
2. Next packet with SYN=1, ACK=1 (server SYN-ACK)

### CDN vs Legacy

Both should have similar handshake times (±5ms) unless:
- CDN endpoint is geographically distant
- CDN load balancing adds overhead
- Network routing differs significantly

**If handshake times are similar but throughput differs**, the issue is NOT connection establishment.

---

## Round-Trip Time (RTT)

### What RTT Measures

**Round-Trip Time** is the time for:
```
Client --- packet ---> Server
Client <-- response -- Server
```

Total time: forward propagation + server processing + return propagation

### How TCP Calculates RTT

TCP automatically tracks RTT using:
1. **Segment sent** at time T1
2. **ACK received** at time T2
3. **RTT = T2 - T1**

This is visible in packet captures through tshark's `tcp.analysis.ack_rtt` field.

### Metrics Collected

```python
{
  "samples": 1247,
  "min_ms": 8.23,
  "max_ms": 156.42,
  "avg_ms": 12.45,
  "median_ms": 11.89,
  "stddev_ms": 8.34,
  "p95_ms": 24.67,
  "p99_ms": 45.23
}
```

### Statistical Significance

**Why we use percentiles:**

- **Average**: Can be skewed by outliers
- **Median**: Middle value, more stable
- **P95**: 95% of RTTs are below this (good for SLAs)
- **P99**: Worst-case excluding extreme outliers

**Standard deviation** tells us how variable RTT is:
- Low stddev (< 5ms): Stable path
- High stddev (> 20ms): Variable latency (bufferbloat, queueing)

### Implementation

```bash
tshark -r file.pcap -T fields -Y "tcp.analysis.ack_rtt" \
  -e frame.time_relative \
  -e tcp.analysis.ack_rtt
```

We collect all RTT samples and calculate statistics.

### Interpreting RTT Results

**Absolute values:**
- **<20ms**: Excellent (same datacenter or metro area)
- **20-50ms**: Good (regional)
- **50-100ms**: Moderate (cross-country)
- **>100ms**: High (international or satellite)

**Variability (stddev/mean as %):**
- **<10%**: Very stable
- **10-30%**: Normal variation
- **30-50%**: Moderate bufferbloat
- **>50%**: Severe queueing issues

### RTT vs Throughput

**Key insight:** RTT and throughput are related but not the same.

**Example 1: High RTT, Good Throughput**
```
RTT: 80ms (US East to US West)
Throughput: 150 Mbps
Diagnosis: High latency due to distance, but bandwidth is good
```

**Example 2: Low RTT, Poor Throughput**
```
RTT: 12ms (same region)
Throughput: 25 Mbps
Diagnosis: NOT a latency issue, something else is limiting throughput
```

### CDN vs Legacy Comparison

**If RTT is similar:**
```
Legacy RTT: 12.3ms (±2.1)
CDN RTT: 13.5ms (±2.4)
```

**Conclusion:** Network path quality is similar. If CDN throughput is still poor, the issue is **NOT network latency**.

**If RTT differs significantly:**
```
Legacy RTT: 12.3ms (±2.1)
CDN RTT: 78.4ms (±5.6)
```

**Conclusion:** CDN endpoint may be geographically distant. But this alone doesn't explain low throughput if TCP windows are adequate.

### Why RTT Alone Doesn't Explain Throttling

With modern TCP (window scaling, SACK, etc.):
- **Even 100ms RTT** should achieve 100+ Mbps with proper window sizes
- **TCP can saturate high-latency links** if allowed to

If throughput is poor despite adequate windows, look at **server sending behavior**.

---

## Packet Loss & Retransmissions

### What Retransmissions Mean

TCP is reliable - if a packet is lost, it's retransmitted.

**Types of retransmissions:**

1. **Fast Retransmission**: After 3 duplicate ACKs, sender assumes loss
2. **Timeout Retransmission**: After RTO (Retransmission Timeout) expires
3. **Spurious Retransmission**: False positive (packet wasn't actually lost)

### Metrics Collected

```python
{
  "total_retransmissions": 23,
  "fast_retransmissions": 18,
  "spurious_retransmissions": 2,
  "total_tcp_packets": 8532,
  "retransmission_rate_pct": 0.270
}
```

### Calculating Retransmission Rate

```
Retransmission Rate (%) = (retransmissions / total_tcp_packets) × 100
```

Example:
```
23 retransmissions / 8532 packets = 0.270%
```

### Interpretation

**Retransmission rate thresholds:**
- **<0.5%**: Excellent, negligible loss
- **0.5-1.0%**: Good, minor loss
- **1.0-3.0%**: Moderate, noticeable impact
- **>3.0%**: Poor, significant packet loss

### Impact on Performance

**Each retransmission:**
- Delays data delivery by 1-3× RTT
- May trigger congestion control (window reduction)
- Reduces effective throughput

**Throughput reduction estimate:**
```
Effective Throughput ≈ Ideal Throughput × (1 - RetransRate)
```

Example:
```
Ideal: 100 Mbps
Retrans Rate: 3%
Effective: 100 × (1 - 0.03) = 97 Mbps
```

### Root Causes of Packet Loss

1. **Network congestion**: Routers drop packets when queues fill
2. **Physical issues**: Bad cables, interference
3. **Firewall/middlebox**: Stateful tracking issues
4. **Buffer overflow**: Receiver or intermediate buffers full

### Implementation

```bash
# Count retransmissions
tshark -r file.pcap -Y "tcp.analysis.retransmission" -T fields -e frame.number

# Count fast retransmissions
tshark -r file.pcap -Y "tcp.analysis.fast_retransmission" -T fields -e frame.number

# Count spurious
tshark -r file.pcap -Y "tcp.analysis.spurious_retransmission" -T fields -e frame.number
```

### CDN vs Legacy Comparison

**Scenario 1: Both have low loss**
```
Legacy: 0.23% retransmissions
CDN: 0.19% retransmissions
```
**Conclusion:** Network paths are both healthy. Retransmissions not the issue.

**Scenario 2: CDN has high loss**
```
Legacy: 0.23% retransmissions
CDN: 3.45% retransmissions
```
**Conclusion:** CDN network path has issues. Could be:
- Routing through congested path
- Firewall dropping packets
- CDN infrastructure overload

**This would be a contributing factor, but check if burst pacing is also present.**

### Relationship to Throttling

**Important:** Throttling and packet loss are **separate issues**.

- Throttling = server deliberately delays sending
- Packet loss = network drops packets

You can have:
- Throttling with no loss (typical for CDN rate limiting)
- Loss with no throttling (network congestion)
- Both (unlucky combination)

---

## TCP Flow Control & Issues

### What We Measure

TCP has built-in flow control mechanisms that can indicate problems:

1. **Duplicate ACKs**: Receiver asking for retransmission
2. **Zero Windows**: Receiver buffer full, can't accept data
3. **Resets (RST)**: Connection aborted

### Metrics Collected

```python
{
  "resets": 0,
  "duplicate_acks": 12,
  "zero_windows": 0
}
```

### Duplicate ACKs

**What it means:**
- Receiver got out-of-order packet (gap detected)
- Sends duplicate ACK to signal gap
- After 3 duplicate ACKs → fast retransmission

**Interpretation:**
- **<10**: Normal, minor reordering
- **10-50**: Moderate packet loss/reordering
- **>50**: Significant network issues

**In our context:**
Moderate duplicate ACKs suggest packet loss (see retransmission section).

### Zero Windows

**What it means:**
- Receiver advertises window size = 0
- Receiver buffer is full
- Sender must stop sending until window opens

**Causes:**
- Receiver application not reading data fast enough
- Receiver CPU/memory constrained
- Receiver disk I/O slow

**Interpretation:**
- **0 zero windows**: Receiver keeping up fine
- **1-5**: Brief receiver stalls
- **>5**: Receiver is bottleneck

**In downloads:**
Zero windows on **client** side would indicate:
- Tanium client can't write to disk fast enough
- Client CPU overloaded
- Client buffer management issue

This would limit throughput regardless of server behavior.

### TCP Resets

**What it means:**
- Connection terminated abruptly
- Either side can send RST

**Causes:**
- Firewall killing connection
- Application crash
- Protocol violation
- Port scan detection

**Interpretation:**
- **0 resets**: Clean connection
- **1 reset at end**: Normal close (some implementations)
- **Multiple resets**: Connection instability

### Implementation

```bash
# Duplicate ACKs
tshark -r file.pcap -Y "tcp.analysis.duplicate_ack" -T fields -e frame.number

# Zero windows
tshark -r file.pcap -Y "tcp.analysis.zero_window" -T fields -e frame.number

# Resets
tshark -r file.pcap -Y "tcp.flags.reset == 1" -T fields -e frame.number
```

### CDN vs Legacy

These metrics should be **similar and low** for both if everything is healthy.

**Red flags:**
- CDN has many zero windows: CDN or client receiving issues
- CDN has many resets: Connection instability
- High duplicate ACKs: Network path issues

But these are typically **not the main issue** in throttling scenarios.

---

## Throughput Analysis

### Throughput Over Time

Instead of just average throughput, we measure **how throughput varies** during the transfer.

### Methodology

1. **Divide transfer into time intervals** (default: 1 second)
2. **Sum bytes in each interval**
3. **Calculate throughput per interval**
4. **Analyze distribution**

### Metrics Collected

```python
{
  "interval_seconds": 1,
  "intervals": [
    {"interval": 0, "time_start": 0, "time_end": 1, "bytes": 12582912, "mbps": 100.66},
    {"interval": 1, "time_start": 1, "time_end": 2, "bytes": 11534336, "mbps": 92.27},
    ...
  ],
  "stats": {
    "min_mbps": 15.23,
    "max_mbps": 145.67,
    "avg_mbps": 87.34,
    "median_mbps": 92.11,
    "stddev_mbps": 24.56
  }
}
```

### Calculating Interval Throughput

```python
# For each time interval
bytes_in_interval = sum(packet_sizes where interval_start <= packet_time < interval_end)
mbps = (bytes_in_interval × 8) / (interval_seconds × 1,000,000)
```

### Throughput Variability Analysis

**Coefficient of Variation (CoV):**
```
CoV = (stddev / mean) × 100%
```

**Interpretation:**
- **<10%**: Very consistent throughput
- **10-30%**: Normal variation (TCP dynamics)
- **30-50%**: Moderate variability
- **>50%**: Highly variable (likely throttling or severe network issues)

### Throughput Patterns

**Pattern 1: Consistent (Healthy)**
```
Time (s):  0    1    2    3    4    5
Mbps:    100  102   98  101   99  100
```
Low CoV (~2%), stable sending.

**Pattern 2: Slow Start (Normal TCP)**
```
Time (s):  0    1    2    3    4    5
Mbps:     20   45   85  100  100  100
```
Increases then stabilizes. Normal TCP congestion control.

**Pattern 3: Throttled/Bursty**
```
Time (s):  0    1    2    3    4    5
Mbps:     80   30   85   25   80   30
```
High CoV (~60%), alternating high/low. **Smoking gun for throttling.**

**Pattern 4: Degrading**
```
Time (s):  0    1    2    3    4    5
Mbps:    100   90   75   60   45   30
```
Progressive decrease. Could be buffer filling, congestion, or server slowdown.

### Implementation

```bash
tshark -r file.pcap -T fields \
  -Y "tcp.srcport == SERVER_PORT and tcp.len > 0" \
  -e frame.time_relative \
  -e tcp.len
```

We group packets by time interval and sum TCP payload bytes.

### CDN vs Legacy Patterns

**Typical Legacy pattern:**
```
Stats: avg=145 Mbps, stddev=12 Mbps, CoV=8%
Pattern: Consistent, smooth
```

**Typical CDN throttled pattern:**
```
Stats: avg=45 Mbps, stddev=28 Mbps, CoV=62%
Pattern: Bursty, high variation
```

**Visualization:**
```
Legacy Throughput Over Time:
150 ████████████████████████████████████
120 ████████████████████████████████████
 90 ████████████████████████████████████
 60 ████████████████████████████████████
 30 ████████████████████████████████████
     0    5   10   15   20   25   30   35

CDN Throughput Over Time:
150 █    █    █    █    █    █    █    █
120 █    █    █    █    █    █    █    █
 90 ██  ██  ██  ██  ██  ██  ██  ██  ██
 60 ██  ██  ██  ██  ██  ██  ██  ██  ██
 30 ██████████████████████████████████
     0    5   10   15   20   25   30   35
```

High variability = throttling signature.

---

## Root Cause Metrics

These are the **key metrics** that identify WHY performance is poor.

---

## Server Response Latency

### What It Measures

**How long the server takes to respond after receiving a request.**

This is **NOT RTT** - it's measuring server-side processing and response initiation time.

### Methodology

1. **Client sends request** (e.g., HTTP GET) at time T1
2. **Server receives request** (approximately T1 + RTT/2)
3. **Server processes and sends data** at time T2
4. **Client receives first data** at T3

**Server Response Latency** ≈ T3 - T1 - RTT

But in practice, we simplify by measuring time from client request to first server data packet.

### Example Scenario

```
T=0ms:   Client sends GET request
T=12ms:  Server receives request (RTT/2)
T=15ms:  Server processes, prepares response
T=20ms:  Server sends first data packet
T=32ms:  Client receives first data (RTT/2)

Server latency ≈ 32ms - 0ms = 32ms
Or more accurately: 20ms - 12ms = 8ms processing time
```

### Metrics Collected

```python
{
  "samples": 156,
  "min_ms": 2.34,
  "max_ms": 45.67,
  "avg_ms": 8.23,
  "median_ms": 7.89,
  "p95_ms": 15.23,
  "p99_ms": 28.45,
  "stddev_ms": 6.78
}
```

### Implementation Details

```python
# Pseudocode
client_requests = []  # Packets TO server with TCP data
server_responses = []  # Packets FROM server with TCP data

for each server response at time T:
    find most recent client request before T
    latency = T - request_time
    if latency < 1000ms:  # Sanity check
        record latency
```

We match server data packets to preceding client requests.

### Interpretation

**Latency values:**
- **<5ms**: Excellent, server responds immediately
- **5-15ms**: Good, normal processing
- **15-50ms**: Moderate, some delay
- **>50ms**: High, server slow to respond

**What causes high latency:**
- Server processing overhead (encryption, compression)
- Server under load (CPU/memory pressure)
- Application-level delays (database queries, file I/O)
- Deliberate rate limiting (queuing requests)

### CDN vs Legacy

**Scenario 1: Similar latencies**
```
Legacy: 5.23ms avg
CDN: 6.78ms avg
```
**Conclusion:** Both respond quickly. Initial response not the bottleneck.

**Scenario 2: CDN slower to respond**
```
Legacy: 5.23ms avg
CDN: 42.34ms avg
```
**Conclusion:** CDN is slow to START sending data. Could be:
- CDN processing overhead
- CDN queuing requests
- Geographic distance
- CDN under load

### Relationship to Overall Performance

**Important:** Even if initial response is slow, sustained throughput could still be good IF the server sends continuously after starting.

**Example:**
```
Server latency: 50ms (slow start)
Inter-packet delay: 0.5ms (fast continuous sending)
Result: Brief initial delay, then good throughput
```

But if **both** server latency AND inter-packet delays are high, you have a comprehensive server-side bottleneck.

---

## Inter-Packet Delays

### ⚠️ PRIMARY ROOT CAUSE INDICATOR ⚠️

This is the **most important metric** for diagnosing server-side throttling.

### What It Measures

**Time between consecutive data packets from the server.**

In a download, the server has data queued and ready to send. How fast does it actually send?

### Methodology

```python
# Get all data packets from server with timestamps
packets = get_packets(src_port=SERVER_PORT, tcp_len > 0)

# Calculate delays between consecutive packets
delays = []
for i in range(1, len(packets)):
    delay_ms = (packets[i].time - packets[i-1].time) * 1000
    delays.append(delay_ms)
```

### Example

```
Packet 1 sent at T=0.0000s
Packet 2 sent at T=0.0005s → delay = 0.5ms
Packet 3 sent at T=0.0011s → delay = 0.6ms
Packet 4 sent at T=0.0015s → delay = 0.4ms
...
```

Average delay = 0.5ms → 2000 packets/second → smooth continuous sending

### Metrics Collected

```python
{
  "samples": 2456,
  "min_ms": 0.012,
  "max_ms": 125.45,
  "avg_ms": 15.234,
  "median_ms": 12.456,
  "p95_ms": 35.678,
  "p99_ms": 67.890,
  "stddev_ms": 18.234
}
```

### Why This Is Critical

**This measures server sending rate directly.**

The server has:
- Data in its send buffer (application layer)
- TCP send queue ready to go
- Network available (client ACKs received)

**If delays are large, the server is:**
- Deliberately pausing between sends (rate limiting)
- Slow to queue data (CPU/I/O bound)
- Using inefficient buffering

**This is NOT network latency** because the data is already queued. It's **server behavior**.

### Interpretation

**Delay thresholds:**
- **<0.5ms**: Excellent, wire-speed sending
- **0.5-2ms**: Good, fast continuous sending
- **2-5ms**: Moderate, some pauses
- **5-10ms**: Poor, noticeable delays
- **>10ms**: Very poor, severe throttling

### Real-World Examples

**Example 1: Unthrottled Server**
```
Average delay: 0.523ms
P95: 1.234ms
P99: 2.456ms

Interpretation: Server sending as fast as possible, continuous stream
```

**Example 2: Lightly Throttled Server**
```
Average delay: 3.456ms
P95: 8.123ms
P99: 15.234ms

Interpretation: Some pacing, but not severe
```

**Example 3: Heavily Throttled Server**
```
Average delay: 18.234ms
P95: 45.678ms
P99: 89.123ms

Interpretation: Severe pacing, clear rate limiting
```

### Statistical Analysis

**Coefficient of Variation:**
```
CoV = (stddev / mean) × 100%
```

- High CoV (>100%): Natural variation, bursty TCP
- Moderate CoV (50-100%): Some consistency
- Low CoV (<50%): Very consistent delays (algorithmic pacing)

### CDN vs Legacy Comparison

**The Smoking Gun:**

```
Legacy:
  Average: 0.523ms
  P95: 1.234ms
  Interpretation: Continuous sending

CDN:
  Average: 15.234ms
  P95: 35.678ms
  Ratio: 29.1x slower

Conclusion: CDN server is pausing 29x longer between packets.
           This is NOT network latency (RTT is similar).
           This IS server-side throttling.
```

**Ratio interpretation:**
- **1-2x**: Minor difference, acceptable
- **2-3x**: Noticeable slowdown
- **3-10x**: Significant throttling
- **>10x**: Severe throttling, clear rate limiting

### Why Small TCP Windows Are a Symptom

When the server sends slowly (large inter-packet delays):

1. **Client receives data slowly**
2. **Client's receive buffer doesn't fill**
3. **TCP advertises smaller window** (no need for large window)
4. **Server continues sending slowly** (rate limiter doesn't care about window)

The small window is TCP's **reaction** to slow data arrival, not the cause.

**Proof:** If you increase the client's window, the throughput **doesn't improve** because the server is still rate limiting.

### Relationship to Burst Patterns

Inter-packet delays and burst patterns are related:

**Continuous sending:**
```
Delays: 0.5, 0.6, 0.4, 0.5, 0.5, 0.6 ms
Pattern: Smooth, all delays similar
```

**Burst pacing:**
```
Delays: 0.5, 0.4, 0.6, 20.0, 0.5, 0.4, 0.5, 20.0 ms
Pattern: Fast packets (burst), then long gap, repeat
```

Average might be 5ms, but the **pattern** reveals throttling mechanism.

---

## Burst Pattern Analysis

### ⚠️ PRIMARY THROTTLING INDICATOR ⚠️

This metric **proves algorithmic rate limiting** by detecting burst pacing patterns.

### What It Measures

**Does the server send data in bursts (with gaps) or continuously?**

- **Burst**: Group of packets sent very quickly (<10ms apart)
- **Gap**: Longer pause between bursts (>10ms)

### Methodology

```python
burst_threshold_ms = 10  # Define what's "fast" vs "gap"

delays = calculate_inter_packet_delays()
bursts = []
current_burst = []
gaps = []

for delay in delays:
    if delay < burst_threshold_ms:
        # Fast packet, part of current burst
        current_burst.append(packet)
    else:
        # Gap found - end current burst
        if current_burst:
            bursts.append(len(current_burst))
            gaps.append(delay)
            current_burst = []
```

### Visual Example

**Continuous sending (Legacy):**
```
Time: ||||||||||||||||||||||||||||||||||||||||
Pkts: ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●

Result: 1-2 large bursts (whole transfer is one stream)
```

**Burst pacing (Throttled CDN):**
```
Time: |   |   |   |   |   |   |   |   |   |   |
Pkts: ●●●   ●●●   ●●●   ●●●   ●●●   ●●●   ●●●   ●●●

Result: 8 bursts with ~20ms gaps between them
```

### Metrics Collected

```python
{
  "total_bursts": 675,
  "avg_burst_size": 4.3,
  "min_burst_size": 2,
  "max_burst_size": 12,
  "avg_gap_ms": 20.31,
  "total_packets": 2456
}
```

### Interpretation

**Burst count thresholds:**
- **<10 bursts**: Continuous sending, natural TCP
- **10-50 bursts**: Minor burst behavior
- **50-100 bursts**: Moderate burst pacing
- **>100 bursts**: Clear burst pacing, algorithmic throttling

**Gap timing:**
- **<5ms gaps**: Not really gaps, normal TCP variation
- **5-15ms gaps**: Moderate pacing
- **15-30ms gaps**: Clear pacing
- **>30ms gaps**: Severe pacing

### Why This Proves Throttling

**Natural TCP doesn't create hundreds of consistent bursts.**

Natural TCP might have:
- Initial slow start (one "burst" as cwnd increases)
- Occasional pauses for ACKs (if window fills)
- Random variation

But **500+ bursts with consistent ~20ms gaps** is:
- **Too consistent** to be network effects
- **Too frequent** to be ACK-based flow control
- **Too precise** to be organic TCP dynamics

**This is algorithmic behavior** - a rate limiter releasing data in controlled bursts.

### Rate Limiting Mechanism

Burst pacing typically indicates a **token bucket** algorithm:

```
Token Bucket Algorithm:
1. Bucket holds tokens (credits to send data)
2. Tokens added at fixed rate (e.g., 50 tokens/sec)
3. Each packet costs tokens
4. If bucket empty, wait until tokens accumulate
5. When tokens available, send burst of packets
```

This creates the burst-gap-burst-gap pattern.

### CDN vs Legacy Comparison

**Typical results:**

```
Legacy:
  Total bursts: 8
  Average gap: 125ms
  Interpretation: Essentially continuous, occasional ACK pauses

CDN:
  Total bursts: 675
  Average gap: 20.31ms
  Interpretation: 675 distinct burst-gap cycles

Ratio: 84x more bursts on CDN

Conclusion: CDN is using burst pacing rate limiting.
           This is NOT organic TCP behavior.
```

### Relationship to Throughput

**Impact of burst pacing:**

```
Example:
- Burst size: 5 packets × 1460 bytes = 7300 bytes
- Burst duration: ~1ms
- Gap duration: 20ms
- Cycle time: 21ms

Throughput = 7300 bytes / 0.021 seconds
          = 347,619 bytes/sec
          = 2.78 Mbps
```

Even though the server can send at 100+ Mbps during bursts, the gaps limit average throughput.

### Real-World Example

From actual PCAP analysis:

```
CDN Statistics:
  Total packets: 2456
  Total bursts: 675
  Average burst size: 3.6 packets
  Average gap: 20.31ms

Math:
  2456 packets / 675 bursts = 3.6 packets/burst ✓
  Total bursts × avg gap = 675 × 20.31ms = 13.7 seconds spent in gaps
  Transfer duration: 45 seconds
  Gap percentage: 13.7 / 45 = 30.4% of time spent in gaps

Conclusion: Server spends 30% of time paused, 70% sending.
           This explains low throughput despite good RTT.
```

---

## Pacing Precision Analysis

### ⚠️ ALGORITHMIC THROTTLING INDICATOR ⚠️

This metric proves the delays are **programmatic, not natural**.

### What It Measures

**How uniform/precise are the inter-packet delays?**

Natural systems (TCP, network queues, CPUs) have inherent variation:
- CPU scheduling jitter (microseconds)
- Network queueing variation
- Interrupt handling timing
- TCP timer granularity

**Algorithmic rate limiters are too precise** - they use software timers and have very consistent behavior.

### Methodology

```python
# Get inter-packet delays
delays = calculate_inter_packet_delays()

# Filter to "typical" pacing delays (exclude obvious bursts/gaps)
filtered_delays = [d for d in delays if 0.01 < d < 10]

# Calculate statistical properties
mean = average(filtered_delays)
stddev = standard_deviation(filtered_delays)
coefficient_of_variation = (stddev / mean) × 100

# Check for clustering around specific values
histogram = bucket_delays(filtered_delays, bucket_size=0.1ms)
top_3_buckets = most_common(histogram, 3)
clustering_pct = sum(top_3_buckets) / total × 100
```

### Coefficient of Variation (CoV)

**Definition:**
```
CoV = (standard deviation / mean) × 100%
```

**Interpretation:**
- **Low CoV (<20%)**: Very consistent, little variation
- **Moderate CoV (20-80%)**: Some variation
- **High CoV (>80%)**: High variation, natural behavior

### Delay Clustering

We create a histogram of delays with fine granularity (0.1ms buckets).

**Natural TCP:**
```
Delay Histogram:
0.2-0.3ms: ████ (12%)
0.3-0.4ms: ████████ (23%)
0.4-0.5ms: ██████ (18%)
0.5-0.6ms: ██████ (17%)
0.6-0.7ms: ████ (11%)
... (spread across many buckets)

Top 3 buckets: 23% + 18% + 17% = 58%
```

**Algorithmic pacing:**
```
Delay Histogram:
19.9-20.0ms: ███████████████ (45%)
20.0-20.1ms: ████████████ (35%)
20.1-20.2ms: ████ (12%)
... (concentrated in few buckets)

Top 3 buckets: 45% + 35% + 12% = 92%
```

### Metrics Collected

```python
{
  "samples": 1234,
  "avg_delay_ms": 0.523,
  "stddev_ms": 0.678,
  "coefficient_of_variation": 129.6,
  "top_3_clustering_pct": 58.4
}
```

### Interpretation

**Thresholds for algorithmic behavior:**

```
CoV < 20% AND Clustering > 60%:
  → Algorithmic pacing CONFIRMED

CoV < 40% AND Clustering > 50%:
  → Likely algorithmic pacing

CoV > 80%:
  → Natural TCP behavior
```

### Why This Matters

Even if inter-packet delays seem reasonable (e.g., 5ms average), if they're **too consistent**, that proves it's programmatic.

**Example:**

```
Server A (Natural):
  Delays: 4.2, 5.8, 3.7, 6.1, 4.9, 5.3 ms
  Average: 5.0ms
  CoV: 85%
  Interpretation: Natural variation

Server B (Throttled):
  Delays: 4.9, 5.0, 5.1, 5.0, 5.0, 4.9 ms
  Average: 5.0ms
  CoV: 8%
  Interpretation: Algorithmic timer, rate limiting
```

Both have the same average, but Server B is clearly using software pacing.

### Real-World Example

**Legacy Server:**
```
Pacing Analysis:
  Samples: 2341
  Average delay: 0.523ms
  Std Dev: 0.678ms
  CoV: 129.6%
  Top 3 clustering: 58.4%

Interpretation:
  High CoV → Natural variation
  Moderate clustering → Delays spread across range
  Conclusion: Organic TCP sending behavior
```

**CDN Server (Throttled):**
```
Pacing Analysis:
  Samples: 1987
  Average delay: 20.31ms (excluding bursts)
  Std Dev: 3.45ms
  CoV: 17.0%
  Top 3 clustering: 78.2%

Interpretation:
  Low CoV → Very consistent timing
  High clustering → 78% of delays in just 3 buckets
  Conclusion: Algorithmic pacing, software rate limiter
```

### The Smoking Gun

When you see:
- **Low CoV** (<20%)
- **High clustering** (>70%)
- **Consistent gap timing** (~20ms)

You can definitively say: **"This is programmatic rate limiting, not network effects or natural TCP dynamics."**

### Implementation Details

```bash
tshark -r file.pcap -T fields \
  -Y "tcp.srcport == SERVER_PORT and tcp.len > 0" \
  -e frame.time_relative
```

Post-processing in Python:
```python
# Calculate delays
delays = []
for i in range(1, len(timestamps)):
    delay_ms = (timestamps[i] - timestamps[i-1]) * 1000
    delays.append(delay_ms)

# Filter to typical pacing range
filtered = [d for d in delays if 0.01 < d < 10]

# Create histogram
histogram = defaultdict(int)
bucket_size = 0.1  # ms
for delay in filtered:
    bucket = round(delay / bucket_size) * bucket_size
    histogram[bucket] += 1

# Find top clusters
sorted_buckets = sorted(histogram.items(), key=lambda x: x[1], reverse=True)
top_3 = sorted_buckets[:3]
top_3_pct = sum(count for _, count in top_3) / len(filtered) * 100
```

---

## Statistical Methods

### Why Statistics Matter

With 5 iterations of each test (Legacy and CDN), we need to:
1. **Aggregate** results properly
2. **Quantify variation** across iterations
3. **Determine statistical significance** of differences

### Measures of Central Tendency

**Mean (Average):**
```
mean = sum(values) / count(values)
```

Pros: Uses all data points
Cons: Sensitive to outliers

**Median:**
```
median = middle value when sorted
```

Pros: Not affected by outliers
Cons: Ignores magnitude of extreme values

**When to use:**
- Use **mean** for normally distributed data
- Use **median** when outliers are present or distribution is skewed

### Measures of Variability

**Standard Deviation (stddev):**
```
variance = sum((value - mean)²) / (count - 1)
stddev = sqrt(variance)
```

Interpretation:
- **Low stddev**: Consistent across iterations
- **High stddev**: Variable results

**Coefficient of Variation (CoV):**
```
CoV = (stddev / mean) × 100%
```

Interpretation:
- **<10%**: Very consistent
- **10-30%**: Moderate variation
- **>30%**: High variation

**Why CoV is useful:**
It's **scale-independent**. You can compare:
- Throughput (Mbps) with CoV = 15%
- RTT (ms) with CoV = 20%

And say "throughput is more consistent than RTT"

### Percentiles

**Definition:** The Pth percentile is the value below which P% of observations fall.

**P95 (95th percentile):**
- 95% of values are below this
- Only 5% are higher
- Good for "worst-case" SLAs

**P99 (99th percentile):**
- 99% of values are below this
- Only 1% are higher
- Useful for identifying rare outliers

**Calculation:**
```python
def percentile(data, p):
    sorted_data = sorted(data)
    index = (len(sorted_data) - 1) * p / 100
    floor_index = int(index)

    if floor_index + 1 < len(sorted_data):
        # Interpolate
        lower = sorted_data[floor_index]
        upper = sorted_data[floor_index + 1]
        fraction = index - floor_index
        return lower + fraction * (upper - lower)

    return sorted_data[floor_index]
```

### Comparing Two Groups

When comparing CDN vs Legacy:

**Ratio of Means:**
```
ratio = mean_cdn / mean_legacy
```

Example:
```
Legacy inter-packet delay: 0.523ms
CDN inter-packet delay: 15.234ms
Ratio: 15.234 / 0.523 = 29.1

Interpretation: CDN is 29x slower at sending consecutive packets
```

**Difference in Standard Deviations:**

If both have similar stddev, the difference is consistent across iterations.

```
Legacy: 145 Mbps (±12)
CDN: 45 Mbps (±8)

Interpretation: CDN is consistently slower (low variation in both)
```

### Statistical Significance

With only 5 iterations each, we can't do formal hypothesis testing, but we can use heuristics:

**Rule of thumb:** If groups' ranges don't overlap, difference is significant.

```
Legacy: 140-160 Mbps (mean ± 2×stddev)
CDN: 40-60 Mbps (mean ± 2×stddev)

No overlap → Difference is real, not random variation
```

### Aggregating Across Iterations

For directory analysis:

```python
# Collect metrics from all iterations
legacy_throughputs = [145.2, 148.7, 143.1, 149.3, 144.8]
cdn_throughputs = [43.5, 47.2, 41.8, 46.1, 44.3]

# Calculate aggregated statistics
legacy_mean = mean(legacy_throughputs)  # 146.2
legacy_stddev = stddev(legacy_throughputs)  # 2.6

cdn_mean = mean(cdn_throughputs)  # 44.6
cdn_stddev = stddev(cdn_throughputs)  # 2.1

# Report
print(f"Legacy: {legacy_mean:.1f} Mbps (±{legacy_stddev:.1f})")
print(f"CDN: {cdn_mean:.1f} Mbps (±{cdn_stddev:.1f})")
```

---

## Diagnosis Logic

### How the Script Diagnoses Issues

The diagnosis follows a **decision tree** based on metric thresholds.

### Decision Tree

```
Start: Comparing CDN vs Legacy performance

├─ RTT similar?
│  ├─ Yes → Network latency NOT the issue
│  └─ No → Note: CDN path has higher latency, but check other metrics
│
├─ Retransmission rate high?
│  ├─ >3% → ISSUE: Packet loss on network path
│  └─ <1% → Network path is clean
│
├─ Burst count high? (CDN >100 bursts AND >3x Legacy)
│  ├─ Yes → 🔴 CRITICAL: Burst pacing throttling
│  └─ No → Continue checking
│
├─ Pacing precision algorithmic? (CoV <20% AND clustering >60%)
│  ├─ Yes → 🔴 CRITICAL: Algorithmic pacing detected
│  └─ No → Continue checking
│
├─ Inter-packet delay high? (CDN >3x Legacy)
│  ├─ Yes → 🔴 CRITICAL: Server slow at sustained delivery
│  └─ No → Continue checking
│
├─ Server response latency high? (CDN >2x Legacy AND >10ms)
│  ├─ Yes → MEDIUM: Server slow to start responding
│  └─ No → Server responsive
│
└─ Throughput low? (<100 Mbps)
   ├─ Yes → HIGH: Low throughput (SYMPTOM of above issues)
   └─ No → Performance acceptable
```

### Issue Prioritization

**CRITICAL (Root Causes):**
1. Burst pacing (proves rate limiting algorithm)
2. Algorithmic pacing precision (proves programmatic behavior)
3. Excessive inter-packet delays (proves slow sending)

**MEDIUM (Contributing Factors):**
4. High server response latency
5. Packet loss >1%

**SYMPTOM (Outcome):**
6. Low throughput
7. High throughput variability

### Diagnosis Examples

**Case 1: Clear Throttling**
```
Metrics:
  RTT: Legacy 12ms, CDN 13ms ✓ (similar)
  Retrans: Legacy 0.2%, CDN 0.3% ✓ (both low)
  Bursts: Legacy 8, CDN 675 🔴 (84x ratio)
  Pacing CoV: Legacy 128%, CDN 17% 🔴 (algorithmic)
  Inter-packet: Legacy 0.5ms, CDN 15.2ms 🔴 (30x ratio)
  Throughput: Legacy 145 Mbps, CDN 45 Mbps (symptom)

Diagnosis:
  🔴 ROOT CAUSE: Server Burst Pacing/Throttling
  🔴 THROTTLING: Algorithmic Pacing Detected
  🔴 ROOT CAUSE: CDN Server Slow at Sustained Data Delivery

Recommendation:
  Contact Tanium about CDN rate limiting configuration.
  This is NOT a network issue - server is deliberately pacing.
```

**Case 2: Network Path Issue**
```
Metrics:
  RTT: Legacy 12ms, CDN 85ms ⚠️ (7x higher)
  Retrans: Legacy 0.2%, CDN 4.5% 🔴 (high loss)
  Bursts: Legacy 8, CDN 12 ✓ (similar)
  Inter-packet: Legacy 0.5ms, CDN 0.7ms ✓ (similar)
  Throughput: Legacy 145 Mbps, CDN 60 Mbps (reduced)

Diagnosis:
  MEDIUM: High network latency to CDN
  MEDIUM: Elevated packet loss

Recommendation:
  CDN routing issue. Check network path to CDN endpoint.
  Run MTR to identify where packet loss occurs.
  Server behavior is normal, network is the bottleneck.
```

**Case 3: Both Issues**
```
Metrics:
  RTT: Legacy 12ms, CDN 78ms ⚠️ (high)
  Retrans: Legacy 0.2%, CDN 2.3% ⚠️ (moderate)
  Bursts: Legacy 8, CDN 450 🔴 (56x ratio)
  Inter-packet: Legacy 0.5ms, CDN 12.3ms 🔴 (25x ratio)
  Throughput: Legacy 145 Mbps, CDN 25 Mbps (poor)

Diagnosis:
  🔴 ROOT CAUSE: Server Burst Pacing/Throttling
  MEDIUM: Elevated packet loss
  MEDIUM: Higher network latency

Recommendation:
  PRIMARY: CDN server throttling
  SECONDARY: CDN network path also suboptimal
  Fix server throttling first (biggest impact).
```

---

## Rate Limiting Algorithms

Understanding HOW rate limiting works helps interpret our metrics.

### Token Bucket Algorithm

**Most common** rate limiting approach.

**How it works:**
```
1. Bucket holds tokens (capacity C)
2. Tokens added at rate R per second
3. To send X bytes, consume X tokens
4. If tokens available:
     - Consume tokens
     - Send data immediately
5. If tokens not available:
     - Wait until tokens accumulate
     - Send when sufficient tokens
```

**Example:**
```
Rate: 50 Mbps = 6,250,000 bytes/sec
Bucket capacity: 100,000 bytes
Token rate: 6,250,000 bytes/sec = 6,250 bytes/ms

Timeline:
T=0ms:  Bucket has 100,000 tokens (full)
        Send 100,000 bytes in burst
        Bucket now has 0 tokens

T=1ms:  Tokens added: 6,250
        Send 6,250 bytes

T=2ms:  Tokens added: 6,250
        Send 6,250 bytes

... continuous at token rate ...

If packets queued faster than token rate:
T=10ms: Need 50,000 bytes, only 6,250 tokens available
        Wait until T=18ms (accumulate 50,000 tokens)
        Creates 8ms GAP
        Send 50,000 bytes in BURST
```

**Result:** Burst-gap-burst-gap pattern

**Signature in metrics:**
- High burst count
- Consistent gap timing
- Low pacing CoV

### Leaky Bucket Algorithm

**Alternative** approach, smoother output.

**How it works:**
```
1. Bucket holds queued data (capacity C)
2. Data "leaks" out at constant rate R
3. Incoming data fills bucket
4. If bucket full, new data dropped/delayed
```

**Example:**
```
Rate: 50 Mbps = 6,250 bytes/ms
Bucket capacity: 100,000 bytes

Timeline:
T=0ms:  100KB arrives, bucket fills to 100KB
T=1ms:  6,250 bytes leak out
T=2ms:  6,250 bytes leak out
... smooth constant rate ...
```

**Result:** More consistent inter-packet delays

**Signature in metrics:**
- Fewer distinct bursts
- Very consistent inter-packet delays
- Very low pacing CoV (<10%)

### Fair Queuing

**Sophisticated** approach for multi-user systems.

Divides bandwidth fairly among active connections.

**Impact on single connection:**
- Can behave like token bucket
- Rate depends on number of active users
- Variable performance

### Detection Strategy

**Token Bucket indicators:**
- Burst count: >100
- Gap timing: Consistent (±20%)
- Pacing CoV: 15-30%

**Leaky Bucket indicators:**
- Burst count: <50
- Gap timing: Very consistent (±5%)
- Pacing CoV: <10%

**No Rate Limiting:**
- Burst count: <10
- Gap timing: Highly variable
- Pacing CoV: >80%

---

## CDN vs Legacy Comparison

### Comparison Methodology

When analyzing a directory with multiple PCAPs:

1. **Categorize files** by filename:
   - Contains "legacy" → Legacy group
   - Contains "cdn" → CDN group

2. **Analyze each file** independently

3. **Aggregate metrics** across iterations:
   - Calculate mean ± stddev for each metric
   - Per group (Legacy and CDN)

4. **Compare groups**:
   - Calculate ratios (CDN/Legacy)
   - Identify significant differences

5. **Diagnose**:
   - Apply decision tree logic
   - Generate recommendations

### Expected Legacy Baseline

**Healthy Legacy server should show:**

```
Connection:
  Handshake time: <20ms

Network:
  RTT: <30ms
  Retransmission rate: <0.5%

Server behavior:
  Inter-packet delay: <1ms average
  Burst count: <20
  Pacing CoV: >80%

Throughput:
  Average: >100 Mbps
  CoV: <20%
```

### CDN Comparison Scenarios

**Scenario A: CDN Performing Well**
```
All metrics within 20% of Legacy
Conclusion: CDN equivalent to Legacy, no issues
```

**Scenario B: CDN Path Latency**
```
RTT: 2-3x Legacy
Retrans: Similar
Server behavior: Similar
Throughput: 70-80% of Legacy

Conclusion: Geographic distance or routing, but server behavior normal
```

**Scenario C: CDN Throttled (Most Common)**
```
RTT: Similar
Retrans: Similar
Burst count: 10-100x Legacy
Inter-packet delay: 10-30x Legacy
Pacing CoV: <20% (vs Legacy >80%)
Throughput: 30-50% of Legacy

Conclusion: Clear server-side throttling
```

### Quantifying the Impact

**Throughput loss calculation:**

```
Legacy throughput: 145 Mbps
CDN throughput: 45 Mbps

Throughput loss: 145 - 45 = 100 Mbps (69% reduction)
```

**Time impact on downloads:**

```
File size: 1 GB

Legacy download time:
  1,000 MB × 8 bits/byte / (145 Mbps) = 55.2 seconds

CDN download time:
  1,000 MB × 8 bits/byte / (45 Mbps) = 177.8 seconds

Additional time: 177.8 - 55.2 = 122.6 seconds (3.2x longer)
```

### Real-World Example

**Actual analysis output:**

```
[LEGACY] Summary across 5 captures:
  Throughput: 146.2 Mbps (±2.6)
  RTT: 12.3 ms (±1.2)
  Retransmission Rate: 0.23% (±0.05%)
  Inter-Packet Delay: 0.52 ms (±0.08)
  Burst Count: 8 bursts (±2)
  Burst Gap: 125.1 ms (±23.4)
  Pacing CoV: 128.3% (±15.2)
  Pacing Clustering: 58.4% (±6.7)

[CDN] Summary across 5 captures:
  Throughput: 44.6 Mbps (±2.1)
  RTT: 13.5 ms (±1.8)
  Retransmission Rate: 0.19% (±0.04%)
  Inter-Packet Delay: 15.23 ms (±1.34)
  Burst Count: 675 bursts (±89)
  Burst Gap: 20.3 ms (±2.1)
  Pacing CoV: 17.4% (±3.2)
  Pacing Clustering: 78.2% (±5.3)

DIAGNOSIS:
🔴 ROOT CAUSE: Server Burst Pacing/Throttling
   CDN bursts: 675 vs Legacy: 8 (84.4x more)
   CDN gap: 20.3ms vs Legacy: 125.1ms (consistent small gaps)
   - CDN server sends data in many small bursts with gaps
   - This is characteristic of algorithmic rate limiting
   - Token bucket with ~50 Mbps limit

🔴 THROTTLING: Algorithmic Pacing Detected
   CoV: 17.4% vs Legacy: 128.3% (7.4x more precise)
   Clustering: 78.2% vs Legacy: 58.4% (delays concentrated)
   - Inter-packet delays are too precise and clustered
   - This precision indicates programmatic rate limiting

🔴 ROOT CAUSE: CDN Server Slow at Sustained Data Delivery
   CDN delay: 15.23ms vs Legacy: 0.52ms (29.3x slower)
   - The CDN server pauses 29x longer between packets
   - NOT network latency (RTT is similar: 13.5ms vs 12.3ms)
   - Server-side rate limiting confirmed

RECOMMENDATIONS:
  1. Contact Tanium support about CDN rate limiting configuration
  2. Request per-connection bandwidth limit increase (currently ~45 Mbps)
  3. Investigate if CDN has tiering (paid vs free performance)
  4. Short-term workaround: Use multiple parallel connections to bypass per-connection limit
  5. Consider requesting CDN bypass option for high-performance customers

IMPACT:
  - 69% throughput reduction (146 Mbps → 45 Mbps)
  - 3.3x longer download times
  - Consistent across all 5 test iterations
  - Affects all CDN users (not environment-specific)
```

---

## Case Studies

### Case Study 1: Token Bucket Throttling

**Scenario:** CDN download via Tanium client, 1GB file

**Observations:**
```
Basic Stats:
  Duration: 178 seconds
  Throughput: 44.9 Mbps

Network Metrics:
  RTT: 14.2ms
  Retransmissions: 0.18%
  (Both healthy)

Server Metrics:
  Inter-packet delay: 14.7ms avg
  Burst count: 823
  Avg burst size: 3.2 packets
  Avg gap: 18.5ms
  Pacing CoV: 19.3%
  Clustering: 74.6%
```

**Analysis:**

1. **Network is healthy**: Low RTT, minimal packet loss
2. **Burst pattern is extreme**: 823 bursts in 178 seconds = 4.6 bursts/second
3. **Gap timing very consistent**: 18.5ms ± 19.3% → most gaps 15-22ms
4. **Burst size consistent**: ~3 packets (4,380 bytes) per burst
5. **Delays clustered**: 74.6% in top 3 buckets → algorithmic

**Diagnosis:**

Token bucket rate limiter with:
- Rate limit: ~45 Mbps
- Bucket size: ~4,400 bytes
- Token refill: ~4,400 bytes per 18.5ms = 237,838 bytes/sec = 1.9 Mbps per burst

Math check:
- 4.6 bursts/sec × 4,400 bytes/burst × 8 bits/byte = 162,000 bits/sec = 0.162 Mbps
- Wait, that's wrong...

Actually:
- Total transfer: 1 GB in 178 sec = 44.9 Mbps ✓
- Burst capacity: allows bursts up to 4,400 bytes
- But average rate enforced at 45 Mbps

**Smoking gun:** The 18.5ms gap is the token bucket refill time.

### Case Study 2: Network Path Issues

**Scenario:** CDN endpoint routed through suboptimal path

**Observations:**
```
Basic Stats:
  Duration: 95 seconds
  Throughput: 84.2 Mbps

Network Metrics:
  RTT: 76.4ms (vs Legacy 12.3ms)
  Retransmissions: 3.2% (vs Legacy 0.2%)
  Duplicate ACKs: 87

Server Metrics:
  Inter-packet delay: 0.89ms
  Burst count: 15
  Pacing CoV: 95.3%
```

**Analysis:**

1. **RTT is 6x higher**: Indicates distant or routed path
2. **Packet loss is elevated**: 3.2% retransmission rate
3. **But server behavior is normal**: Low inter-packet delay, few bursts, high CoV

**Diagnosis:**

Network path bottleneck, NOT server throttling.

**Recommendations:**
- Investigate CDN endpoint selection (possibly routed internationally)
- Check for network congestion or routing issues
- Run traceroute/MTR to identify hop causing delays
- Server is behaving normally, so focus on network

### Case Study 3: Perfect Storm (Both Issues)

**Scenario:** Throttled CDN over poor network path

**Observations:**
```
Basic Stats:
  Duration: 285 seconds
  Throughput: 28.1 Mbps

Network Metrics:
  RTT: 68.3ms
  Retransmissions: 2.1%

Server Metrics:
  Inter-packet delay: 22.3ms
  Burst count: 1,247
  Pacing CoV: 14.2%
```

**Analysis:**

Both issues present:
- High RTT + packet loss = network path problem
- High burst count + low CoV = server throttling

**Diagnosis:**

Multiple bottlenecks compounding:
1. **Primary**: Server throttling (~40-50 Mbps limit)
2. **Secondary**: Poor network path (reduces effective rate further)

**Impact breakdown:**
```
Ideal throughput: 150 Mbps (wire speed)
After network path: ~100 Mbps (RTT/loss impact)
After throttling: 28 Mbps (rate limiter enforced)

Network accounts for ~33% loss
Throttling accounts for ~72% loss
Combined: 81% loss
```

**Recommendations:**
1. Fix server throttling first (biggest impact)
2. Then address network path issues
3. Don't assume network is the only problem

---

## Troubleshooting Guide

### Quick Diagnostic Flowchart

```
Is throughput poor (<100 Mbps)?
├─ NO → Performance acceptable, no action needed
└─ YES ↓

Is RTT high (>50ms)?
├─ YES → Network latency is a factor
│        Check routing, endpoint selection
└─ NO ↓

Is retransmission rate high (>1%)?
├─ YES → Packet loss is a factor
│        Check network path, firewalls
└─ NO ↓

Is burst count high (>100)?
├─ YES → 🔴 Server throttling detected
│        Check pacing precision
└─ NO ↓

Is inter-packet delay high (>5ms)?
├─ YES → 🔴 Server sending slowly
│        Investigate server performance
└─ NO ↓

Unclear cause - review all metrics
```

### Common Misdiagnoses

**Mistake 1: "The network is slow"**

**Evidence:**
- Low throughput
- Small TCP windows

**Actual cause:**
- Server throttling with burst pacing
- Windows are small BECAUSE server sends slowly

**How to confirm:**
- Check RTT (is it actually high?)
- Check inter-packet delays (server pausing?)
- Check burst patterns (algorithmic?)

---

**Mistake 2: "We need bigger TCP windows"**

**Evidence:**
- Small receive windows advertised
- Low throughput

**Actual cause:**
- Server throttling limits sending rate
- Client doesn't need big window if server sends slowly

**How to confirm:**
- Check inter-packet delays (if >5ms, server is the bottleneck)
- Increasing window won't help if server is rate limiting

---

**Mistake 3: "It's packet loss"**

**Evidence:**
- Some retransmissions
- Variable throughput

**Actual cause:**
- Minor packet loss (<1%) is normal
- Throughput variability due to burst pacing

**How to confirm:**
- Check retransmission rate (if <1%, not the main issue)
- Check burst patterns (high variability = throttling)

---

**Mistake 4: "The CDN is just far away"**

**Evidence:**
- Higher RTT to CDN
- Lower throughput

**Actual cause:**
- RTT accounts for some loss
- But burst pacing is the main factor

**How to confirm:**
- Calculate BDP (Bandwidth-Delay Product)
- With 50ms RTT and 64KB window, can still achieve 10+ Mbps
- If throughput is much lower, RTT isn't the only issue

### Interpreting Conflicting Signals

**Scenario:** Good network metrics, poor throughput

```
RTT: 15ms ✓
Retrans: 0.3% ✓
Throughput: 35 Mbps ✗
```

**Diagnosis:** Server-side issue (check burst patterns, inter-packet delays)

---

**Scenario:** Poor network metrics, good server behavior

```
RTT: 85ms ⚠️
Retrans: 3.5% ⚠️
Inter-packet delay: 0.7ms ✓
Burst count: 12 ✓
```

**Diagnosis:** Network path issue (server is fine, route is bad)

---

**Scenario:** Everything looks OK, but throughput is low

```
RTT: 12ms ✓
Retrans: 0.2% ✓
Inter-packet delay: 1.2ms ✓ (seems OK)
Burst count: 45 ⚠️ (moderate)
Throughput: 60 Mbps ⚠️
```

**Diagnosis:** Mild throttling (not severe, but present). Check pacing precision for algorithmic behavior.

### When to Escalate

**Contact Network Team:**
- High RTT (>50ms) to CDN endpoint
- High packet loss (>1%) consistently
- Routing issues visible in traceroute

**Contact Vendor (Tanium):**
- Burst pacing confirmed (>100 bursts)
- Algorithmic pacing precision (CoV <20%)
- Inter-packet delays 5x+ higher than baseline
- Consistent across all tests

**Both:**
- Multiple issues compounding
- Unclear root cause after analysis

---

## Appendix: Technical Details

### Tshark Commands Reference

**Basic packet info:**
```bash
tshark -r file.pcap -T fields \
  -e frame.number \
  -e frame.time_relative \
  -e frame.len
```

**TCP specific:**
```bash
tshark -r file.pcap -T fields -Y tcp \
  -e tcp.srcport \
  -e tcp.dstport \
  -e tcp.len \
  -e tcp.window_size \
  -e tcp.flags.syn \
  -e tcp.flags.ack
```

**RTT analysis:**
```bash
tshark -r file.pcap -T fields \
  -Y "tcp.analysis.ack_rtt" \
  -e frame.time_relative \
  -e tcp.analysis.ack_rtt
```

**Retransmissions:**
```bash
tshark -r file.pcap -Y "tcp.analysis.retransmission"
tshark -r file.pcap -Y "tcp.analysis.fast_retransmission"
```

**Data packets from server:**
```bash
tshark -r file.pcap -T fields \
  -Y "tcp.srcport == 443 and tcp.len > 0" \
  -e frame.time_relative \
  -e tcp.len
```

### Python Processing Examples

**Calculate inter-packet delays:**
```python
import subprocess

def get_packet_times(pcap_file, server_port):
    cmd = [
        'tshark', '-r', pcap_file,
        '-T', 'fields',
        '-Y', f'tcp.srcport == {server_port} and tcp.len > 0',
        '-e', 'frame.time_relative'
    ]

    result = subprocess.run(cmd, capture_output=True, text=True)
    times = []

    for line in result.stdout.strip().split('\n')[1:]:  # Skip header
        if line:
            times.append(float(line))

    return times

def calculate_delays(times):
    delays = []
    for i in range(1, len(times)):
        delay_ms = (times[i] - times[i-1]) * 1000
        delays.append(delay_ms)
    return delays

# Usage
times = get_packet_times('capture.pcap', 443)
delays = calculate_delays(times)

import statistics
print(f"Average delay: {statistics.mean(delays):.3f} ms")
print(f"Median delay: {statistics.median(delays):.3f} ms")
print(f"P95 delay: {percentile(delays, 95):.3f} ms")
```

**Burst detection:**
```python
def detect_bursts(delays, threshold_ms=10):
    bursts = []
    current_burst = []
    gaps = []

    for i, delay in enumerate(delays):
        if delay < threshold_ms:
            current_burst.append(i)
        else:
            if current_burst:
                bursts.append(len(current_burst) + 1)  # +1 for first packet
                gaps.append(delay)
                current_burst = []

    if current_burst:
        bursts.append(len(current_burst) + 1)

    return bursts, gaps

# Usage
bursts, gaps = detect_bursts(delays)
print(f"Total bursts: {len(bursts)}")
print(f"Average burst size: {statistics.mean(bursts):.1f} packets")
print(f"Average gap: {statistics.mean(gaps):.2f} ms")
```

### Calculating Bandwidth-Delay Product (BDP)

**Formula:**
```
BDP = Bandwidth × RTT
```

**Purpose:** Maximum amount of data "in flight" on the network.

**Example:**
```
Bandwidth: 100 Mbps = 12,500,000 bytes/sec
RTT: 50ms = 0.05 seconds

BDP = 12,500,000 × 0.05 = 625,000 bytes = 610 KB
```

**Interpretation:**

TCP window must be ≥ BDP to fully utilize bandwidth.

With 64 KB window (common default):
```
Max throughput = Window / RTT
              = 65,536 bytes / 0.05 sec
              = 1,310,720 bytes/sec
              = 10.5 Mbps
```

Even with 100 Mbps available, limited to 10.5 Mbps by window size.

**But:** With window scaling (typical now), windows can be much larger (megabytes), so this is rarely the bottleneck unless RTT is very high (>200ms).

---

## Summary

This guide has covered:

1. **TCP Fundamentals** - How TCP works in download scenarios
2. **All Metrics** - What each measures and why it matters
3. **Statistical Methods** - How to interpret and aggregate data
4. **Diagnosis Logic** - How to identify root causes
5. **Rate Limiting** - How throttling algorithms work
6. **Comparison Methods** - How to compare CDN vs Legacy
7. **Case Studies** - Real-world examples
8. **Troubleshooting** - How to diagnose issues correctly

### Key Takeaways

**For Performance Analysis:**
- Look at **root causes** (server behavior) not just symptoms (throughput)
- **RTT and throughput are different** - low RTT doesn't guarantee high throughput
- **TCP windows are reactive** - small windows are a symptom, not a cause

**For Throttling Detection:**
- **Burst patterns** (>100 bursts) prove rate limiting
- **Pacing precision** (CoV <20%, clustering >60%) proves algorithmic behavior
- **Inter-packet delays** (10x+ higher than baseline) prove slow sending

**For Vendor Discussions:**
- Use **data** not assumptions ("here's proof of burst pacing")
- Be **specific** ("675 bursts with 20ms gaps, CoV 17%")
- Request **configuration review** ("per-connection rate limits")

### Complete Workflow

**Step 1: Generate PCAPs**
```bash
# Run the performance test to generate packet captures
python3 tanium_download_perf_test.py

# This will create a directory: tanium_pcaps_YYYYMMDD_HHMMSS/
# Containing 5 legacy and 5 CDN download captures
```

**Step 2: Analyze PCAPs**
```bash
# Analyze the entire directory (recommended)
python3 analyze_tanium_pcaps.py --pcap-dir tanium_pcaps_YYYYMMDD_HHMMSS/

# This produces:
# - Console output with comparison and diagnosis
# - JSON file: pcap_analysis_<timestamp>.json with detailed metrics

# Or analyze a single file
python3 analyze_tanium_pcaps.py --pcap-file legacy_iteration_1.pcap --server-port 17472
python3 analyze_tanium_pcaps.py --pcap-file cdn_iteration_1.pcap --server-port 443
```

**Step 3: Interpret Results**
- Review the comparison summary (CDN vs Legacy)
- Check the diagnosis section for root causes
- Use this guide to understand each metric
- Share results with Tanium support if throttling detected

### Quick Reference

**Script**: `analyze_tanium_pcaps.py`
**Input**: PCAPs from `tanium_download_perf_test.py`
**Output**: Console analysis + JSON file
**Requirements**: tshark (Wireshark CLI)

---

**Document Version:** 2.0
**Last Updated:** 2025
**Script:** analyze_tanium_pcaps.py
**PCAP Source:** tanium_download_perf_test.py
**Author:** Network Performance Analysis Team
